{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Instalación de requerimientos (Python 3.9.6)\n",
    "!pip install google-play-scraper sentence-transformers matplotlib seaborn pandas feedparser transformers nltk wordcloud pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import feedparser\n",
    "\n",
    "from google_play_scraper import reviews, app, Sort\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "# Importar pysentimiento para el análisis de sentimiento\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener reviews desde Google Play Store\n",
    "def get_playstore_reviews(app_id, lang='es', country='AR', days_back=90):\n",
    "    result, _ = reviews(\n",
    "        app_id,\n",
    "        lang=lang,\n",
    "        country=country,\n",
    "        sort=Sort.NEWEST,  # Usamos el enum correcto\n",
    "        count=2000\n",
    "    )\n",
    "    df = pd.DataFrame(result)\n",
    "    df['date'] = pd.to_datetime(df['at'])\n",
    "    df.rename(columns={'score': 'rating'}, inplace=True)\n",
    "    cutoff = dt.datetime.now() - dt.timedelta(days=days_back)\n",
    "    return df[df['date'] >= cutoff]\n",
    "\n",
    "# Función para obtener reviews desde App Store vía RSS de iTunes\n",
    "def get_itunes_reviews(app_store_id, days_back=90):\n",
    "    # URL del RSS de reseñas de iTunes (formato JSON)\n",
    "    url = f\"https://itunes.apple.com/rss/customerreviews/id/{app_store_id}/json\"\n",
    "    feed = feedparser.parse(url)\n",
    "    reviews_list = []\n",
    "    # El primer entry suele ser metadata, por lo que se omite\n",
    "    for entry in feed.entries[1:]:\n",
    "        # Validar que la entrada tenga el campo de rating\n",
    "        if 'im_rating' in entry:\n",
    "            try:\n",
    "                rating = int(entry['im_rating'])\n",
    "            except:\n",
    "                rating = None\n",
    "        else:\n",
    "            rating = None\n",
    "        title = entry.get('title', '')\n",
    "        # El contenido suele venir en 'content' o en 'summary'\n",
    "        if 'content' in entry and len(entry.content) > 0:\n",
    "            content = entry.content[0].value\n",
    "        else:\n",
    "            content = entry.get('summary', '')\n",
    "        review_date = pd.to_datetime(entry.get('updated'))\n",
    "        reviews_list.append({\n",
    "            'date': review_date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'content': content\n",
    "        })\n",
    "    df = pd.DataFrame(reviews_list)\n",
    "    if df.empty or \"date\" not in df.columns:\n",
    "        return df\n",
    "    cutoff = dt.datetime.now() - dt.timedelta(days=days_back)\n",
    "    df = df[df['date'] >= cutoff]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un diccionario con los IDs de cada app\n",
    "apps = {\n",
    "    \"belo\": {\n",
    "         \"play_store_id\": \"com.belo.android\",\n",
    "         \"app_store_id\": \"1575614708\"\n",
    "    },\n",
    "    \"astropay\": {\n",
    "         \"play_store_id\": \"com.astropaycard.android\",\n",
    "         \"app_store_id\": \"1128476912\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lista para almacenar los DataFrames de reviews\n",
    "all_reviews_list = []\n",
    "\n",
    "# Iterar sobre cada app y obtener reviews de ambas tiendas\n",
    "for app_name, ids in apps.items():\n",
    "    # Reviews desde Google Play\n",
    "    play_reviews = get_playstore_reviews(ids[\"play_store_id\"])\n",
    "    if not play_reviews.empty:\n",
    "        play_reviews[\"app\"] = app_name\n",
    "        play_reviews[\"store\"] = \"Play Store\"\n",
    "        all_reviews_list.append(play_reviews)\n",
    "    \n",
    "    # Reviews desde App Store (iTunes RSS)\n",
    "    itunes_reviews = get_itunes_reviews(ids[\"app_store_id\"])\n",
    "    if not itunes_reviews.empty:\n",
    "        itunes_reviews[\"app\"] = app_name\n",
    "        itunes_reviews[\"store\"] = \"App Store\"\n",
    "        all_reviews_list.append(itunes_reviews)\n",
    "\n",
    "# Concatenar todas las reviews en un único DataFrame\n",
    "all_reviews = pd.concat(all_reviews_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de sentimiento con pysentimiento\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    result = analyzer.predict(text)\n",
    "    return result.output\n",
    "\n",
    "all_reviews[\"sentiment\"] = all_reviews[\"content\"].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de embeddings y pipeline de sentimiento\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n",
    "\n",
    "# Frases clave representativas para cada categoría\n",
    "keywords = {\n",
    "    \"bug\": [\"error\", \"problemas\", \"bug\", \"no abre\", \"no funciona\", \"no me deja\", \"no puedo\", \"no anda\", \"no carga\"],\n",
    "    \"feature\": [\"sería bueno\", \"me gustaría que tenga\", \"necesito que agreguen\", \"falta\", \"sumen\", \"es mejor\", \n",
    "        \"prefiero\", \"me gustaría que tenga\", \"sería genial si agregaran\", \"quisiera que se incluya\", \"necesito que ofrezcan\"]\n",
    "}\n",
    "\n",
    "# Pre-codificar las keywords\n",
    "keyword_embeddings = {k: model.encode(v, convert_to_tensor=True) for k, v in keywords.items()}\n",
    "\n",
    "def classify_keywords_with_sentiment(text, threshold=0.6, bug_strict_threshold=0.7, feature_strict_threshold=0.75):\n",
    "    \"\"\"\n",
    "    Evalúa la similitud entre la review y las frases clave, combinándola con el análisis de sentimiento.\n",
    "    \n",
    "    Se ajustan los umbrales en función del sentimiento obtenido:\n",
    "    \n",
    "    - Para bugs:\n",
    "      * Si el sentimiento es NEG, se permite un umbral ligeramente menor (threshold - 0.1).\n",
    "      * Si es POS, se exige un umbral mayor (bug_strict_threshold).\n",
    "      * Si es NEU, se utiliza el umbral base (threshold).\n",
    "      \n",
    "    - Para feature requests:\n",
    "      * Si el sentimiento es POS, se exige un umbral mayor (feature_strict_threshold).\n",
    "      * Si es NEG o NEU, se utiliza el umbral base (threshold).\n",
    "      \n",
    "    Retorna una serie con dos valores booleanos: \"is_bug\" e \"is_feature\".\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return pd.Series({\"is_bug\": False, \"is_feature\": False})\n",
    "    \n",
    "    # Obtener sentimiento (por ejemplo: \"POS\", \"NEG\", \"NEU\")\n",
    "    sentiment_result = sentiment_pipeline(text)[0]\n",
    "    sentiment_label = sentiment_result['label'].upper()\n",
    "    \n",
    "    # Calcular embedding del texto\n",
    "    text_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    scores = {}\n",
    "    for category, emb_list in keyword_embeddings.items():\n",
    "        sim = util.cos_sim(text_embedding, emb_list)\n",
    "        scores[category] = sim.max().item()\n",
    "    \n",
    "    # Ajuste del umbral para bugs según el sentimiento\n",
    "    if sentiment_label == \"NEG\":\n",
    "        bug_threshold = threshold - 0.1  # umbral más permisivo para bugs en reviews negativas\n",
    "    elif sentiment_label == \"POS\":\n",
    "        bug_threshold = bug_strict_threshold  # se exige mayor similitud si el sentimiento es positivo\n",
    "    else:\n",
    "        bug_threshold = threshold\n",
    "    \n",
    "    is_bug = scores[\"bug\"] > bug_threshold\n",
    "    \n",
    "    # Ajuste del umbral para features según el sentimiento\n",
    "    if sentiment_label == \"POS\":\n",
    "        feature_threshold = feature_strict_threshold  # se exige mayor similitud para features en reviews positivas\n",
    "    else:\n",
    "        feature_threshold = threshold  # para NEU y NEG se usa el umbral base\n",
    "    \n",
    "    is_feature = scores[\"feature\"] > feature_threshold\n",
    "    \n",
    "    return pd.Series({\"is_bug\": is_bug, \"is_feature\": is_feature})\n",
    "\n",
    "\n",
    "tags = all_reviews[\"content\"].apply(classify_keywords_with_sentiment)\n",
    "all_reviews = pd.concat([all_reviews, tags], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesa un texto: lo pasa a minúsculas, elimina caracteres no alfabéticos\n",
    "    (conservando acentos y ñ) y reduce espacios múltiples.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres no alfabéticos (conservar letras con acento y ñ)\n",
    "    text = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", text)\n",
    "    # Eliminar espacios extra\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_topics(reviews, n_topics=5, n_top_words=10, min_df=2, max_df=0.95):\n",
    "    \"\"\"\n",
    "    Extrae los temas más recurrentes de un listado de reviews utilizando LDA.\n",
    "    \n",
    "    Parámetros:\n",
    "      reviews (list o pd.Series): Lista o serie de textos de reviews.\n",
    "      n_topics (int): Número de temas a extraer.\n",
    "      n_top_words (int): Número de palabras clave que se mostrarán por tema.\n",
    "      min_df (int): Frecuencia mínima para que una palabra se incluya.\n",
    "      max_df (float): Fracción máxima de documentos en la que una palabra puede aparecer.\n",
    "      \n",
    "    Retorna:\n",
    "      dict: Un diccionario donde las keys son nombres de temas y los valores son listas con las palabras clave.\n",
    "    \"\"\"\n",
    "    # Preprocesar los reviews válidos\n",
    "    preprocessed_reviews = [preprocess_text(text) for text in reviews if isinstance(text, str) and text.strip() != '']\n",
    "    if not preprocessed_reviews:\n",
    "        return {}\n",
    "    \n",
    "    # Usar las stop words en español de NLTK\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    \n",
    "    # Vectorizar el texto usando CountVectorizer con parámetros ajustados\n",
    "    vectorizer = CountVectorizer(stop_words=spanish_stopwords, min_df=min_df, max_df=max_df)\n",
    "    X = vectorizer.fit_transform(preprocessed_reviews)\n",
    "    \n",
    "    # Verificar que se hayan obtenido términos\n",
    "    if X.shape[1] == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Aplicar LDA para extraer temas\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = {}\n",
    "    \n",
    "    # Extraer las n_top_words de cada tema\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topics[f\"Tema {topic_idx+1}\"] = top_features\n",
    "        \n",
    "    return topics\n",
    "\n",
    "\n",
    "for sentiment in all_reviews['sentiment'].unique():\n",
    "    subset = all_reviews[all_reviews['sentiment'] == sentiment]['content']\n",
    "    print(f\"\\nTópicos para reviews con sentimiento {sentiment}:\")\n",
    "    topics = extract_topics(subset, n_topics=5, n_top_words=10)\n",
    "    for topic, words in topics.items():\n",
    "        print(f\"{topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregación semanal de las reviews\n",
    "all_reviews['week'] = pd.to_datetime(all_reviews['date']).dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "\n",
    "# Asegurarse de que las columnas 'sentiment', 'is_bug' e 'is_feature' existan\n",
    "for col in ['sentiment', 'is_bug', 'is_feature']:\n",
    "    if col not in all_reviews.columns:\n",
    "         all_reviews[col] = None  \n",
    "\n",
    "weekly_summary = all_reviews.groupby(['app', 'store', 'week']).agg(\n",
    "    avg_rating=('rating', 'mean'),\n",
    "    review_count=('rating', 'count'),\n",
    "    positive_sentiment=('sentiment', lambda x: (x.str.contains(\"POS\", case=False, na=False)).sum()),\n",
    "    negative_sentiment=('sentiment', lambda x: (x.str.contains(\"NEG\", case=False, na=False)).sum()),\n",
    "    feature_requests=('is_feature', 'sum'),\n",
    "    bug_reports=('is_bug', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por app y sentimiento\n",
    "sentiment_counts_app = all_reviews.groupby(['app', 'sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='sentiment', y='Count', hue='app', data=sentiment_counts_app, palette='viridis')\n",
    "plt.title(\"Distribución de Sentimientos por App\")\n",
    "plt.xlabel(\"Sentimiento\")\n",
    "plt.ylabel(\"Cantidad de Reviews\")\n",
    "plt.show()\n",
    "\n",
    "# Agrupar por semana, app y sentimiento\n",
    "weekly_sentiment_app = all_reviews.groupby(['week', 'app', 'sentiment']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='week', y='Count', hue='sentiment', style='app', data=weekly_sentiment_app, markers=True)\n",
    "plt.title(\"Evolución Semanal de los Sentimientos por App\")\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Cantidad de Reviews\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Sentimiento / App\")\n",
    "plt.show()\n",
    "\n",
    "# Mapear el sentimiento a un score numérico\n",
    "sentiment_map = {\"POS\": 1, \"NEU\": 0, \"NEG\": -1}\n",
    "all_reviews[\"sentiment_score\"] = all_reviews[\"sentiment\"].map(sentiment_map)\n",
    "\n",
    "# Visualización: Rating vs Sentiment Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Boxplot para mostrar la distribución de 'sentiment_score' por rating\n",
    "sns.boxplot(x=\"rating\", y=\"sentiment_score\", data=all_reviews, hue=\"app\", showfliers=False)\n",
    "# Stripplot para mostrar cada punto\n",
    "sns.stripplot(x=\"rating\", y=\"sentiment_score\", data=all_reviews, hue=\"app\", \n",
    "              dodge=True, alpha=0.5, color='black', jitter=True)\n",
    "\n",
    "plt.title(\"Distribución de Sentiment Score por Rating (posible ironía)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.legend(title=\"App\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Visualizar los tópicos por sentimiento usando WordCloud\n",
    "for sentiment in all_reviews['sentiment'].unique():\n",
    "    subset = all_reviews[all_reviews['sentiment'] == sentiment]['content']\n",
    "    topics = extract_topics(subset, n_topics=5, n_top_words=10)\n",
    "    \n",
    "    # Combinar las palabras clave de todos los tópicos en una única cadena\n",
    "    all_words = []\n",
    "    for topic, words in topics.items():\n",
    "         all_words.extend(words)\n",
    "    text = \" \".join(all_words)\n",
    "    \n",
    "    # Generar el WordCloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud de tópicos para reviews con sentimiento {sentiment}\")\n",
    "    plt.show()\n",
    "\n",
    "# Listado de Feature Requests\n",
    "features_reviews = all_reviews[all_reviews['is_feature'] == True]\n",
    "print(\"Listado de Feature Requests:\")\n",
    "display(features_reviews[['date', 'app', 'store', 'content', 'sentiment']].sort_values(by='date', ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Listado de Bug Reports para la app 'belo'\n",
    "bugs_reviews = all_reviews[(all_reviews['is_bug'] == True) & (all_reviews['app'] == 'belo')]\n",
    "print(\"Listado de Bug Reports:\")\n",
    "display(bugs_reviews[['date', 'app', 'store', 'content', 'sentiment']].sort_values(by='date', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar todas las reviews con análisis de sentimiento\n",
    "all_reviews.to_csv(\"all_reviews.csv\", index=False)\n",
    "\n",
    "# Guardar el resumen semanal\n",
    "weekly_summary.to_csv(\"weekly_summary.csv\", index=False)\n",
    "\n",
    "# Guardar feature requests\n",
    "features_reviews.to_csv(\"features_reviews.csv\", index=False)\n",
    "\n",
    "# Guardar bug reports\n",
    "bugs_reviews.to_csv(\"bugs_reviews.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
